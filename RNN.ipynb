{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958a631a-c3dc-4def-9421-ba8542383313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a00d13-79af-4c1e-a446-2ad15dd67b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('CS 583 Project/jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv('CS 583 Project/validation.csv')\n",
    "test = pd.read_csv('CS 583 Project/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4ddaef-6c6d-49a2-aac2-b540308c9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n",
    "train = train.loc[:12000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58373ae4-7c50-444a-9b41-454f47ba8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train['comment_text'], train['toxic'], stratify=train['toxic'], random_state=42, test_size=0.2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d49303f-902f-4e80-9a2d-ba8d62d91ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('CS 583 Project/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74def43-dd9b-430c-ae83-8be1db9a22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, max_vocab_size=20000):\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        word_counter.update(text.split())\n",
    "    most_common = word_counter.most_common(max_vocab_size)\n",
    "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd87655-0741-478d-8c77-9d0918c3fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[idx] = glove_vectors[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a8e08d-e841-4a6b-ba09-2c525f5e1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, vocab, max_len=150):\n",
    "    words = text.split()\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in words[:max_len]]\n",
    "    sequence += [vocab[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac1b140-8328-4a3d-b154-5a11686806a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq = xtrain.apply(lambda x: text_to_sequence(x, vocab))\n",
    "xvalid_seq = xvalid.apply(lambda x: text_to_sequence(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e2203e9-a16b-40f0-b6fe-5573238d4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.sequences.iloc[idx], dtype=torch.long),\n",
    "            \"target\": torch.tensor(self.targets.iloc[idx], dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9215ab4d-0cc9-4a86-9ff1-4f76ce8395fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(xtrain_seq, ytrain)\n",
    "valid_dataset = ToxicCommentsDataset(xvalid_seq, yvalid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e36c00c-79b0-4973-88a7-686c15dc5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentRNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=128, num_layers=1):\n",
    "        super(ToxicCommentRNNClassifier, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83bbf61-b86c-46fd-a139-fac848871a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToxicCommentRNNClassifier(embedding_matrix)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a9aad1-790d-478d-948e-6fd327632efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.3053610878934463\n",
      "Validation AUC: 0.534108547552371\n",
      "Validation Accuracy: 0.9058725531028738\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 0.004405286343612335\n",
      "Validation F1-Score: 0.008771929824561403\n",
      "Confusion Matrix:\n",
      "[[2174    0]\n",
      " [ 226    1]]\n",
      "Epoch 2/3, Training Loss: 0.3034588119635979\n",
      "Validation AUC: 0.51777717437558\n",
      "Validation Accuracy: 0.905039566847147\n",
      "Validation Precision: 0.3333333333333333\n",
      "Validation Recall: 0.004405286343612335\n",
      "Validation F1-Score: 0.008695652173913044\n",
      "Confusion Matrix:\n",
      "[[2172    2]\n",
      " [ 226    1]]\n",
      "Epoch 3/3, Training Loss: 0.30067873276770113\n",
      "Validation AUC: 0.5180872060271774\n",
      "Validation Accuracy: 0.9042065805914202\n",
      "Validation Precision: 0.0\n",
      "Validation Recall: 0.0\n",
      "Validation F1-Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[2171    3]\n",
      " [ 227    0]]\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                targets = batch[\"target\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids).squeeze()\n",
    "                valid_preds.extend(outputs.cpu().numpy().flatten().tolist())  # Fix: Flatten and convert to list\n",
    "                valid_targets.extend(targets.cpu().numpy().tolist())\n",
    "\n",
    "        # Threshold for binary classification\n",
    "        valid_preds_binary = np.array(valid_preds) > 0.5\n",
    "\n",
    "        # Metrics\n",
    "        auc = roc_auc_score(valid_targets, valid_preds)\n",
    "        acc = accuracy_score(valid_targets, valid_preds_binary)\n",
    "        precision = precision_score(valid_targets, valid_preds_binary)\n",
    "        recall = recall_score(valid_targets, valid_preds_binary)\n",
    "        f1 = f1_score(valid_targets, valid_preds_binary)\n",
    "        cm = confusion_matrix(valid_targets, valid_preds_binary)\n",
    "\n",
    "        print(f\"Validation AUC: {auc}\")\n",
    "        print(f\"Validation Accuracy: {acc}\")\n",
    "        print(f\"Validation Precision: {precision}\")\n",
    "        print(f\"Validation Recall: {recall}\")\n",
    "        print(f\"Validation F1-Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "\n",
    "\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7dd23-2c1b-4ca9-80aa-865a764b874e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f5537-06e8-4514-a5a3-e96359676edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
