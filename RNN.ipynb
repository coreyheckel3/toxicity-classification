{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958a631a-c3dc-4def-9421-ba8542383313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a00d13-79af-4c1e-a446-2ad15dd67b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('CS 583 Project/jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv('CS 583 Project/validation.csv')\n",
    "test = pd.read_csv('CS 583 Project/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4ddaef-6c6d-49a2-aac2-b540308c9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n",
    "train = train.loc[:12000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58373ae4-7c50-444a-9b41-454f47ba8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train['comment_text'], train['toxic'], stratify=train['toxic'], random_state=42, test_size=0.2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d49303f-902f-4e80-9a2d-ba8d62d91ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('CS 583 Project/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a74def43-dd9b-430c-ae83-8be1db9a22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, max_vocab_size=20000):\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        word_counter.update(text.split())\n",
    "    most_common = word_counter.most_common(max_vocab_size)\n",
    "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd87655-0741-478d-8c77-9d0918c3fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[idx] = glove_vectors[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a8e08d-e841-4a6b-ba09-2c525f5e1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, vocab, max_len=150):\n",
    "    words = text.split()\n",
    "    sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in words[:max_len]]\n",
    "    sequence += [vocab[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bac1b140-8328-4a3d-b154-5a11686806a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq = xtrain.apply(lambda x: text_to_sequence(x, vocab))\n",
    "xvalid_seq = xvalid.apply(lambda x: text_to_sequence(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e2203e9-a16b-40f0-b6fe-5573238d4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=150):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        sequence = text_to_sequence(text, self.vocab, self.max_len)\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9215ab4d-0cc9-4a86-9ff1-4f76ce8395fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(xtrain_seq, ytrain)\n",
    "valid_dataset = ToxicCommentsDataset(xvalid_seq, yvalid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e36c00c-79b0-4973-88a7-686c15dc5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        final_hidden_state = rnn_out[:, -1, :]  # Last hidden state\n",
    "        output = self.fc(final_hidden_state)\n",
    "        return self.sigmoid(output).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c83bbf61-b86c-46fd-a139-fac848871a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToxicCommentRNNClassifier(embedding_matrix)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7a9aad1-790d-478d-948e-6fd327632efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.3059838726868232\n",
      "Validation AUC: 0.5249616006549165\n",
      "Validation Accuracy: 0.9058725531028738\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 0.004405286343612335\n",
      "Validation F1-Score: 0.008771929824561403\n",
      "Confusion Matrix:\n",
      "[[2174    0]\n",
      " [ 226    1]]\n",
      "Epoch 2/3, Training Loss: 0.3031545462956031\n",
      "Validation AUC: 0.5363648890167741\n",
      "Validation Accuracy: 0.9046230737192836\n",
      "Validation Precision: 0.25\n",
      "Validation Recall: 0.004405286343612335\n",
      "Validation F1-Score: 0.008658008658008658\n",
      "Confusion Matrix:\n",
      "[[2171    3]\n",
      " [ 226    1]]\n",
      "Epoch 3/3, Training Loss: 0.301729512338837\n",
      "Validation AUC: 0.5178095959862046\n",
      "Validation Accuracy: 0.9025406080799667\n",
      "Validation Precision: 0.18181818181818182\n",
      "Validation Recall: 0.00881057268722467\n",
      "Validation F1-Score: 0.01680672268907563\n",
      "Confusion Matrix:\n",
      "[[2165    9]\n",
      " [ 225    2]]\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                targets = batch[\"target\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids).squeeze()\n",
    "                valid_preds.extend(outputs.cpu().numpy().flatten().tolist())  # Fix: Flatten and convert to list\n",
    "                valid_targets.extend(targets.cpu().numpy().tolist())\n",
    "\n",
    "        # Threshold for binary classification\n",
    "        valid_preds_binary = np.array(valid_preds) > 0.5\n",
    "\n",
    "        # Metrics\n",
    "        auc = roc_auc_score(valid_targets, valid_preds)\n",
    "        acc = accuracy_score(valid_targets, valid_preds_binary)\n",
    "        precision = precision_score(valid_targets, valid_preds_binary, zero_division=1)\n",
    "        recall = recall_score(valid_targets, valid_preds_binary)\n",
    "        f1 = f1_score(valid_targets, valid_preds_binary)\n",
    "        cm = confusion_matrix(valid_targets, valid_preds_binary)\n",
    "\n",
    "        print(f\"Validation AUC: {auc}\")\n",
    "        print(f\"Validation Accuracy: {acc}\")\n",
    "        print(f\"Validation Precision: {precision}\")\n",
    "        print(f\"Validation Recall: {recall}\")\n",
    "        print(f\"Validation F1-Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "\n",
    "\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3d53b3f-b2c9-43e1-9b9f-2ec9555dadcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: No\n",
      "toxic: No\n",
      "toxic: No\n"
     ]
    }
   ],
   "source": [
    "def predict_toxicity(comment, model, vocab, max_len=150):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert comment to a sequence\n",
    "        sequence = text_to_sequence(comment, vocab, max_len)\n",
    "        input_ids = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # Predict the toxicity\n",
    "        outputs = model(input_ids)\n",
    "        prediction = outputs.cpu().numpy().flatten()\n",
    "\n",
    "        # Convert predictions to binary (threshold = 0.5)\n",
    "        prediction_binary = (prediction > 0.5).astype(int)\n",
    "\n",
    "    # Display the result\n",
    "    categories = ['toxic']  # Update the categories if you are predicting multiple labels\n",
    "    for category, pred in zip(categories, prediction_binary):\n",
    "        print(f\"{category}: {'Yes' if pred == 1 else 'No'}\")\n",
    "\n",
    "# Test the function\n",
    "test_comment = \"You're such a horrible person!\"\n",
    "predict_toxicity(test_comment, model, vocab)\n",
    "\n",
    "test_comment1 = \"You should go to hell!\"\n",
    "predict_toxicity(test_comment1, model, vocab)\n",
    "\n",
    "test_comment2 = \"You are amazing!\"\n",
    "predict_toxicity(test_comment2, model, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b620b-2083-4e11-a7e4-99c26a7efa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a50d3-79b3-4a7d-8ae7-2cbb98c4deb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
