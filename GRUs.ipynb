{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28259d1-3cf1-425f-a9f0-d2e11b4125c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300257 unique tokens.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8327s\u001b[0m 3s/step - accuracy: 0.9159 - loss: 0.0907 - val_accuracy: 0.9951 - val_loss: 0.0532\n",
      "Epoch 2/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1177s\u001b[0m 421ms/step - accuracy: 0.9941 - loss: 0.0499 - val_accuracy: 0.9942 - val_loss: 0.0511\n",
      "Epoch 3/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1103s\u001b[0m 395ms/step - accuracy: 0.9917 - loss: 0.0431 - val_accuracy: 0.9938 - val_loss: 0.0540\n",
      "Epoch 4/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2154s\u001b[0m 771ms/step - accuracy: 0.9887 - loss: 0.0373 - val_accuracy: 0.9949 - val_loss: 0.0558\n",
      "Epoch 5/5\n",
      "\u001b[1m2795/2795\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8940s\u001b[0m 3s/step - accuracy: 0.9614 - loss: 0.0324 - val_accuracy: 0.9905 - val_loss: 0.0577\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 47ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.74      0.76      0.75      4266\n",
      " severe_toxic       0.65      0.12      0.20       384\n",
      "      obscene       0.77      0.77      0.77      2486\n",
      "       threat       0.55      0.23      0.32       133\n",
      "       insult       0.73      0.66      0.69      2294\n",
      "identity_hate       0.66      0.30      0.41       408\n",
      "\n",
      "    micro avg       0.74      0.69      0.71      9971\n",
      "    macro avg       0.68      0.47      0.52      9971\n",
      " weighted avg       0.74      0.69      0.70      9971\n",
      "  samples avg       0.07      0.06      0.06      9971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/Users/shacheesb/Downloads/jigsaw-toxic-comment-train.csv')  # Replace with your dataset path\n",
    "\n",
    "# Check for missing values and drop them\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract features and labels\n",
    "texts = data['comment_text']\n",
    "labels = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=20000)  # Limit to top 20,000 words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Found {len(word_index)} unique tokens.\")\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 200\n",
    "data_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=128, input_length=max_sequence_length),\n",
    "    Bidirectional(GRU(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(GRU(32)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(6, activation='sigmoid')  # Sigmoid for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_predictions = model.predict(X_val)\n",
    "val_predictions = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, val_predictions, target_names=labels.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44868033-2680-4844-a7a6-77d59a3940d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "toxic: Yes\n",
      "severe_toxic: No\n",
      "obscene: No\n",
      "threat: No\n",
      "insult: No\n",
      "identity_hate: No\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "toxic: Yes\n",
      "severe_toxic: No\n",
      "obscene: No\n",
      "threat: No\n",
      "insult: Yes\n",
      "identity_hate: No\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and predict toxic categories\n",
    "def predict_toxicity(comment):\n",
    "    # Tokenize and pad the input comment\n",
    "    sequence = tokenizer.texts_to_sequences([comment])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Predict the toxic categories\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    \n",
    "    # Convert predictions to binary (threshold = 0.5)\n",
    "    prediction_binary = (prediction > 0.5).astype(int)\n",
    "    \n",
    "    # Display results\n",
    "    categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    for category, pred in zip(categories, prediction_binary[0]):\n",
    "        print(f\"{category}: {'Yes' if pred == 1 else 'No'}\")\n",
    "\n",
    "# Example comment to test\n",
    "test_comment = \"You're such a horrible person!\"\n",
    "predict_toxicity(test_comment)\n",
    "test_comment1 = \"You should go to hell!\"\n",
    "predict_toxicity(test_comment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9084b82-dae7-4ad3-8110-f1e56bd08a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
